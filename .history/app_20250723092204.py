from flask import Flask, render_template, request, jsonify, send_file
import os
import threading
import time
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import pandas as pd
import random
import re

app = Flask(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çŠ¶æ…‹ã‚’ç®¡ç†
scraping_status = {
    'is_running': False,
    'progress': 0,
    'total': 0,
    'current_url': '',
    'message': '',
    'filename': '',
    'completed': False
}

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’ä¿å­˜
scraping_results = {
    'shop_data': [],
    'staff_data': [],
    'timestamp': None
}

def get_shop_urls():
    url = "https://www.host2.jp/shop/1_all.html"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        shop_urls = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            if '/shop/' in href and '/index.html' in href:
                if not href.startswith('http'):
                    href = f"https://www.host2.jp{href}"
                shop_urls.append(href)
        
        return list(set(shop_urls))
    
    except Exception as e:
        return []

def get_shop_info(shop_url, include_staff=False):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        time.sleep(random.uniform(1, 2))
        response = requests.get(shop_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # åº—èˆ—åã‚’å–å¾—
        path_nav = soup.find('nav', class_='path')
        if not path_nav:
            return None
        
        shop_name = path_nav.find_all('li')[-2].text.strip()
        
        # é›»è©±ç•ªå·ã‚’å–å¾—
        tel = None
        tel_pattern = re.compile(r'TEL:(\d{2,4}-\d{2,4}-\d{4})')
        for script in soup.find_all('script'):
            if script.string:
                match = tel_pattern.search(script.string)
                if match:
                    tel = match.group(1)
                    break
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—
        system_url = shop_url.replace('/index.html', '/system.html')
        system_response = requests.get(system_url, headers=headers, timeout=10)
        system_response.encoding = 'utf-8'
        system_soup = BeautifulSoup(system_response.text, 'html.parser')
        
        system_table = system_soup.find('div', class_='shop-system-tbl')
        if not system_table:
            return None
        
        info = {
            'åº—èˆ—å': shop_name,
            'åº—èˆ—URL': shop_url,
            'é›»è©±ç•ªå·': tel if tel else 'æœªè¨­å®š'
        }
        
        # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º
        event_info = extract_event_info(soup)
        if event_info:
            info['ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±'] = event_info
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
        for row in system_table.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) >= 2:
                key = cells[0].text.strip()
                value = cells[1].text.strip()
                if key == 'å–¶æ¥­æ™‚é–“':
                    info['å–¶æ¥­æ™‚é–“'] = value
                elif key == 'å®šä¼‘æ—¥':
                    info['å®šä¼‘æ—¥'] = value
                elif key == 'åˆå›æ–™é‡‘':
                    info['åˆå›æ–™é‡‘'] = value
        
        # ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if include_staff:
            staff_info = get_staff_info(shop_url, headers)
            if staff_info:
                info['ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±'] = staff_info
            else:
                # ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’æ¢ã™
                main_page_staff = extract_staff_from_main_page(soup)
                if main_page_staff:
                    info['ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±'] = main_page_staff
                    print(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’å–å¾—: {shop_url}")
        
        return info
    
    except Exception as e:
        return None

def get_staff_info(shop_url, headers):
    """ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’å–å¾—ã™ã‚‹é–¢æ•°"""
    try:
        # ã¾ãšã‚¹ã‚¿ãƒƒãƒ•ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰å€‹åˆ¥ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        staff_links = get_staff_links(shop_url, headers)
        
        if staff_links:
            # ã‚¹ãƒãƒ›ç‰ˆã‹ã‚‰ç›´æ¥å–å¾—ã—ãŸå ´åˆã¯ãã®ã¾ã¾è¿”ã™
            if isinstance(staff_links, list) and staff_links and 'ã‚½ãƒ¼ã‚¹' in staff_links[0]:
                return staff_links
            # å€‹åˆ¥ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—
            return get_staff_details_from_links(staff_links, headers)
        
        # ã‚¹ã‚¿ãƒƒãƒ•ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å¾“æ¥ã®æ–¹æ³•ã‚’è©¦ã™
        staff_url_patterns = [
            shop_url.replace('/index.html', '/staff.html'),
            shop_url.replace('/index.html', '/member.html'),
            shop_url.replace('/index.html', '/cast.html'),
            shop_url.replace('/index.html', '/girls.html'),
            shop_url.replace('/index.html', '/staff/'),
            shop_url.replace('/index.html', '/member/'),
            shop_url.replace('/index.html', '/cast/'),
            shop_url.replace('/index.html', '/girls/'),
        ]
        
        # å„URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        for staff_url in staff_url_patterns:
            try:
                time.sleep(random.uniform(0.3, 0.8))
                response = requests.get(staff_url, headers=headers, timeout=10)
                response.encoding = 'utf-8'
                
                if response.status_code == 200:
                    break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                elif response.status_code == 404:
                    continue  # 404ã®å ´åˆã¯æ¬¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
                else:
                    response.raise_for_status()
            except requests.exceptions.RequestException:
                continue  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ¬¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        else:
            # ã™ã¹ã¦ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤±æ•—ã—ãŸå ´åˆ
            print(f"ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {shop_url} (è©¦è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³: {len(staff_url_patterns)}å€‹)")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        staff_list = []
        
        # ã‚ˆã‚Šå¤šãã®ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        staff_selectors = [
            # ä¸€èˆ¬çš„ãªã‚¹ã‚¿ãƒƒãƒ•ã‚³ãƒ³ãƒ†ãƒŠ
            ['div', 'section'],
            # ã‚ˆã‚Šå…·ä½“çš„ãªã‚¯ãƒ©ã‚¹å
            ['div', 'section', 'article'],
            # ç”»åƒãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢
            ['img'],
        ]
        
        for selector_type in staff_selectors:
            if selector_type == ['img']:
                # ç”»åƒãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢
                staff_images = soup.find_all('img', src=lambda x: x and any(keyword in x.lower() for keyword in ['staff', 'member', 'cast', 'girl', 'photo', 'image']))
                for img in staff_images:
                    parent = img.find_parent(['div', 'section', 'article'])
                    if parent:
                        staff_name = parent.find(['h3', 'h4', 'p', 'span', 'div'])
                        if staff_name:
                            staff_text = parent.get_text().strip()
                            height = extract_height(staff_text)
                            staff_list.append({
                                'åå‰': staff_name.text.strip(),
                                'ç”»åƒURL': img.get('src', ''),
                                'èº«é•·': height,
                                'è©³ç´°': staff_text[:200] + '...' if len(staff_text) > 200 else staff_text
                            })
            else:
                # ã‚¯ãƒ©ã‚¹åãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢
                staff_containers = soup.find_all(selector_type, class_=lambda x: x and any(keyword in x.lower() for keyword in ['staff', 'member', 'cast', 'girl', 'profile', 'info']))
                
                for container in staff_containers:
                    staff_items = container.find_all(['div', 'article', 'section'], recursive=False)
                    if not staff_items:
                        # ç›´æ¥ã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†
                        staff_text = container.get_text().strip()
                        name_elem = container.find(['h3', 'h4', 'p', 'span', 'div'])
                        img_elem = container.find('img')
                        
                        if name_elem:
                            height = extract_height(staff_text)
                            staff_list.append({
                                'åå‰': name_elem.text.strip(),
                                'ç”»åƒURL': img_elem.get('src', '') if img_elem else '',
                                'èº«é•·': height,
                                'è©³ç´°': staff_text[:200] + '...' if len(staff_text) > 200 else staff_text
                            })
                    else:
                        for item in staff_items:
                            name_elem = item.find(['h3', 'h4', 'p', 'span', 'div'])
                            img_elem = item.find('img')
                            
                            if name_elem:
                                staff_text = item.get_text().strip()
                                height = extract_height(staff_text)
                                staff_info = {
                                    'åå‰': name_elem.text.strip(),
                                    'ç”»åƒURL': img_elem.get('src', '') if img_elem else '',
                                    'èº«é•·': height,
                                    'è©³ç´°': staff_text[:200] + '...' if len(staff_text) > 200 else staff_text
                                }
                                staff_list.append(staff_info)
        
        # é‡è¤‡ã‚’é™¤å»ï¼ˆåå‰ã§åˆ¤å®šï¼‰
        unique_staff = []
        seen_names = set()
        for staff in staff_list:
            if staff['åå‰'] not in seen_names:
                unique_staff.append(staff)
                seen_names.add(staff['åå‰'])
        
        return unique_staff if unique_staff else None
        
    except Exception as e:
        print(f"ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ ({shop_url}): {e}")
        return None

def extract_height(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰èº«é•·ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    # èº«é•·ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ï¼ˆã‚ˆã‚Šå¤šãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
    height_patterns = [
        r'èº«é•·[ï¼š:]\s*(\d{3})cm',  # èº«é•·ï¼š183cm
        r'èº«é•·[ï¼š:]\s*(\d{3})ã',  # èº«é•·ï¼š183ã
        r'èº«é•·[ï¼š:]\s*(\d{3})',    # èº«é•·ï¼š183
        r'T[ï¼š:]\s*(\d{3})cm',     # T: 183cm
        r'T[ï¼š:]\s*(\d{3})ã',     # T: 183ã
        r'T[ï¼š:]\s*(\d{3})',       # T: 183
        r'èº«é•·\s*(\d{3})cm',       # èº«é•· 183cm
        r'èº«é•·\s*(\d{3})ã',       # èº«é•· 183ã
        r'èº«é•·\s*(\d{3})',         # èº«é•· 183
        r'T\s*(\d{3})cm',          # T 183cm
        r'T\s*(\d{3})ã',          # T 183ã
        r'T\s*(\d{3})',            # T 183
        r'(\d{3})cm',              # 183cm
        r'(\d{3})ã',              # 183ãï¼ˆå…¨è§’ï¼‰
        r'(\d{3})ã‚»ãƒ³ãƒ',          # 183ã‚»ãƒ³ãƒ
        r'(\d{3})',                # å˜ç´”ã«3æ¡ã®æ•°å­—ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
    ]
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šèº«é•·ã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ­ã‚°å‡ºåŠ›
    height_keywords = ['èº«é•·', 'cm', 'ã', 'ã‚»ãƒ³ãƒ', 't:', 'tï¼š', '182', '183', '175', 'ä¸€æ˜¥', 'é€Ÿæ°´']
    if any(keyword in text.lower() for keyword in height_keywords):
        print(f"èº«é•·æŠ½å‡ºãƒ‡ãƒãƒƒã‚° - ãƒ†ã‚­ã‚¹ãƒˆ: {text[:200]}...")
    
    for i, pattern in enumerate(height_patterns):
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            try:
                height = int(match.group(1))
                # èº«é•·ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆ140cm-200cmã®ç¯„å›²ã«æ‹¡å¼µï¼‰
                if 140 <= height <= 200:
                    print(f"èº«é•·æŠ½å‡ºæˆåŠŸ: {height}cm (ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}: {pattern}, ãƒãƒƒãƒ: {match.group(0)})")
                    return height
                else:
                    print(f"èº«é•·ç¯„å›²å¤–: {height}cm (ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}: {pattern})")
            except ValueError:
                print(f"æ•°å€¤å¤‰æ›ã‚¨ãƒ©ãƒ¼: {match.group(1)} (ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}: {pattern})")
    
    return None
    
    for pattern in height_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            height = int(match.group(1))
            # èº«é•·ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆ140cm-200cmã®ç¯„å›²ã«æ‹¡å¼µï¼‰
            if 140 <= height <= 200:
                return height
    
    return None

def extract_event_info(soup):
    """ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    try:
        event_info = []
        
        # ã‚¤ãƒ™ãƒ³ãƒˆé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        event_keywords = [
            'åˆå›', 'ãƒ‡ãƒ¼', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³', 'ç‰¹å…¸', 'å‰²å¼•', 
            'ã‚»ãƒ¼ãƒ«', 'æœŸé–“é™å®š', 'æ–°è¦', 'ã‚ªãƒ¼ãƒ—ãƒ³', 'è¨˜å¿µ', 'ç‰¹åˆ¥',
            'ç„¡æ–™', 'æ–™é‡‘', 'ä¾¡æ ¼', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ãƒ—ãƒ©ãƒ³'
        ]
        
        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        page_text = soup.get_text()
        
        # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
        event_elements = soup.find_all(['div', 'section', 'p', 'span'], 
                                      string=lambda text: text and any(keyword in text for keyword in event_keywords))
        
        for element in event_elements:
            text = element.get_text().strip()
            if len(text) > 10 and len(text) < 500:  # é©åˆ‡ãªé•·ã•ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                event_info.append(text)
        
        # æ—¥ä»˜æƒ…å ±ã‚‚æŠ½å‡º
        date_patterns = [
            r'\d{1,2}æœˆ\d{1,2}æ—¥',
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
            r'\d{1,2}/\d{1,2}',
            r'\d{1,2}-\d{1,2}',
            r'ä»Šæ—¥', 'æ˜æ—¥', 'ä»Šæœˆ', 'æ¥æœˆ'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, page_text)
            for match in matches:
                if match not in event_info:
                    event_info.append(f"æ—¥ä»˜: {match}")
        
        return event_info if event_info else None
        
    except Exception as e:
        return None

def extract_staff_from_main_page(soup):
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    try:
        staff_list = []
        
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã§ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’æ¢ã™
        staff_selectors = [
            # ã‚¹ã‚¿ãƒƒãƒ•ç´¹ä»‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            ['div', 'section'],
            # ç”»åƒãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢
            ['img'],
            # å€‹åˆ¥ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ã®æ§‹é€ 
            ['h1', 'h2', 'h3'],
        ]
        
        for selector_type in staff_selectors:
            if selector_type == ['img']:
                # ã‚¹ã‚¿ãƒƒãƒ•ç”»åƒã‚’æ¢ã™
                staff_images = soup.find_all('img', src=lambda x: x and any(keyword in x.lower() for keyword in ['staff', 'member', 'cast', 'girl', 'photo', 'image', 'profile']))
                for img in staff_images:
                    parent = img.find_parent(['div', 'section', 'article'])
                    if parent:
                        staff_name = parent.find(['h3', 'h4', 'p', 'span', 'div'])
                        if staff_name:
                            staff_text = parent.get_text().strip()
                            height = extract_height(staff_text)
                            staff_list.append({
                                'åå‰': staff_name.text.strip(),
                                'ç”»åƒURL': img.get('src', ''),
                                'èº«é•·': height,
                                'è©³ç´°': staff_text[:200] + '...' if len(staff_text) > 200 else staff_text
                            })
            elif selector_type == ['h1', 'h2', 'h3']:
                # å€‹åˆ¥ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ã®æ§‹é€ ã«å¯¾å¿œ
                for tag in ['h1', 'h2', 'h3']:
                    headers = soup.find_all(tag)
                    for header in headers:
                        header_text = header.get_text().strip()
                        # ã‚¹ã‚¿ãƒƒãƒ•åã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
                        if len(header_text) > 2 and len(header_text) < 50:
                            # ãƒ˜ãƒƒãƒ€ãƒ¼ã®å‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                            parent = header.find_parent(['div', 'section', 'article'])
                            if parent:
                                parent_text = parent.get_text().strip()
                                height = extract_height(parent_text)
                                if height:  # èº«é•·ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿è¿½åŠ 
                                    staff_list.append({
                                        'åå‰': header_text,
                                        'ç”»åƒURL': '',
                                        'èº«é•·': height,
                                        'è©³ç´°': parent_text[:300] + '...' if len(parent_text) > 300 else parent_text
                                    })
            else:
                # ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
                staff_containers = soup.find_all(selector_type, class_=lambda x: x and any(keyword in x.lower() for keyword in ['staff', 'member', 'cast', 'girl', 'profile', 'info', 'introduction']))
                
                for container in staff_containers:
                    staff_items = container.find_all(['div', 'article', 'section'], recursive=False)
                    if not staff_items:
                        # ç›´æ¥ã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†
                        staff_text = container.get_text().strip()
                        name_elem = container.find(['h3', 'h4', 'p', 'span', 'div'])
                        img_elem = container.find('img')
                        
                        if name_elem and len(staff_text) > 20:
                            height = extract_height(staff_text)
                            staff_list.append({
                                'åå‰': name_elem.text.strip(),
                                'ç”»åƒURL': img_elem.get('src', '') if img_elem else '',
                                'èº«é•·': height,
                                'è©³ç´°': staff_text[:200] + '...' if len(staff_text) > 200 else staff_text
                            })
                    else:
                        for item in staff_items:
                            name_elem = item.find(['h3', 'h4', 'p', 'span', 'div'])
                            img_elem = item.find('img')
                            
                            if name_elem:
                                staff_text = item.get_text().strip()
                                height = extract_height(staff_text)
                                staff_info = {
                                    'åå‰': name_elem.text.strip(),
                                    'ç”»åƒURL': img_elem.get('src', '') if img_elem else '',
                                    'èº«é•·': height,
                                    'è©³ç´°': staff_text[:200] + '...' if len(staff_text) > 200 else staff_text
                                }
                                staff_list.append(staff_info)
        
        # é‡è¤‡ã‚’é™¤å»ï¼ˆåå‰ã§åˆ¤å®šï¼‰
        unique_staff = []
        seen_names = set()
        for staff in staff_list:
            if staff['åå‰'] not in seen_names:
                unique_staff.append(staff)
                seen_names.add(staff['åå‰'])
        
        return unique_staff if unique_staff else None
        
    except Exception as e:
        print(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_mobile_headers():
    """ã‚¹ãƒãƒ›ç‰ˆã®User-Agentã‚’è¿”ã™"""
    return {
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
    }

def get_staff_links(shop_url, headers):
    """ã‚¹ã‚¿ãƒƒãƒ•ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰å€‹åˆ¥ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—"""
    try:
        # ã‚¹ãƒãƒ›ç‰ˆã¨PCç‰ˆã®ä¸¡æ–¹ã‚’è©¦ã™
        user_agents = [
            get_mobile_headers(),  # ã‚¹ãƒãƒ›ç‰ˆ
            headers  # PCç‰ˆ
        ]
        
        # ã‚¹ã‚¿ãƒƒãƒ•ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        staff_list_patterns = [
            shop_url.replace('/index.html', '/staff.html'),
            shop_url.replace('/index.html', '/member.html'),
            shop_url.replace('/index.html', '/cast.html'),
            shop_url,  # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸è‡ªä½“ãŒã‚¹ã‚¿ãƒƒãƒ•ãƒªã‚¹ãƒˆã®å ´åˆ
        ]
        
        for ua_headers in user_agents:
            for staff_list_url in staff_list_patterns:
                try:
                    time.sleep(random.uniform(0.3, 0.8))
                    response = requests.get(staff_list_url, headers=ua_headers, timeout=10)
                    response.encoding = 'utf-8'
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # ã‚¹ãƒãƒ›ç‰ˆã®å ´åˆã€ã‚¹ã‚¿ãƒƒãƒ•ä¸€è¦§ã‹ã‚‰ç›´æ¥èº«é•·æƒ…å ±ã‚’æŠ½å‡º
                        mobile_staff = extract_mobile_staff_list(soup, staff_list_url, ua_headers == get_mobile_headers())
                        if mobile_staff:
                            print(f"ã‚¹ãƒãƒ›ç‰ˆã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’{len(mobile_staff)}å€‹ç™ºè¦‹: {staff_list_url}")
                            return mobile_staff
                        
                        # ã‚¹ã‚¿ãƒƒãƒ•ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                        staff_links = []
                        
                        # ãƒªãƒ³ã‚¯ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ã‚’æ¢ã™
                        for link in soup.find_all('a', href=True):
                            href = link['href']
                            link_text = link.get_text().strip()
                            
                            # ã‚¹ã‚¿ãƒƒãƒ•åã‚‰ã—ã„ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                            if (len(link_text) > 1 and len(link_text) < 20 and 
                                not any(keyword in link_text.lower() for keyword in ['home', 'top', 'menu', 'staff', 'member', 'cast', 'girls', 'åº—èˆ—', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«', 'ãƒˆãƒ”ãƒƒã‚¯ã‚¹', 'ã‚°ãƒ©ãƒ“ã‚¢', 'å‹•ç”»', 'åˆå›ç‰¹å…¸', 'ãƒ–ãƒ­ã‚°', 'æ±‚äºº'])):
                                
                                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                                if href.startswith('/'):
                                    full_url = f"https://www.host2.jp{href}"
                                elif href.startswith('http'):
                                    full_url = href
                                else:
                                    # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆ
                                    base_url = '/'.join(staff_list_url.split('/')[:-1])
                                    full_url = f"{base_url}/{href}"
                                
                                staff_links.append({
                                    'name': link_text,
                                    'url': full_url
                                })
                        
                        if staff_links:
                            ua_type = "ã‚¹ãƒãƒ›ç‰ˆ" if ua_headers == get_mobile_headers() else "PCç‰ˆ"
                            print(f"{ua_type}ã‚¹ã‚¿ãƒƒãƒ•ãƒªãƒ³ã‚¯ã‚’{len(staff_links)}å€‹ç™ºè¦‹: {staff_list_url}")
                            return staff_links
                            
                except Exception as e:
                    continue
        
        return None
        
    except Exception as e:
        print(f"ã‚¹ã‚¿ãƒƒãƒ•ãƒªãƒ³ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_mobile_staff_list(soup, staff_list_url, is_mobile):
    """ã‚¹ãƒãƒ›ç‰ˆã®ã‚¹ã‚¿ãƒƒãƒ•ä¸€è¦§ã‹ã‚‰ç›´æ¥æƒ…å ±ã‚’æŠ½å‡º"""
    try:
        if not is_mobile:
            return None
            
        staff_list = []
        
        # ã‚¹ãƒãƒ›ç‰ˆç‰¹æœ‰ã®ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±æ§‹é€ ã‚’æ¢ã™
        # ãƒªã‚¹ãƒˆå½¢å¼ã§ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        staff_items = soup.find_all(['li', 'div', 'span'], string=lambda text: text and any(keyword in text for keyword in ['cm', 'ã', 'ã‚»ãƒ³ãƒ']))
        
        for item in staff_items:
            text = item.get_text().strip()
            
            # ã‚¹ã‚¿ãƒƒãƒ•åã¨èº«é•·ã‚’æŠ½å‡º
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: "åå‰ èº«é•·ï¼šXXXcm"
            name_height_patterns = [
                r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+èº«é•·[ï¼š:]\s*(\d{3})cm',
                r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+(\d{3})cm',
                r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+T[ï¼š:]\s*(\d{3})',
                r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+.*?(\d{3})cm',
                r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+.*?(\d{3})ã',
                r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+.*?èº«é•·[ï¼š:\s]*(\d{3})',
            ]
            
            for pattern in name_height_patterns:
                match = re.search(pattern, text)
                if match:
                    name = match.group(1).strip()
                    height = int(match.group(2))
                    
                    if 140 <= height <= 200 and len(name) > 1 and len(name) < 20:
                        staff_info = {
                            'åå‰': name,
                            'èº«é•·': height,
                            'ç”»åƒURL': '',
                            'è©³ç´°': text,
                            'å€‹åˆ¥URL': '',
                            'ã‚½ãƒ¼ã‚¹': 'ã‚¹ãƒãƒ›ç‰ˆä¸€è¦§'
                        }
                        staff_list.append(staff_info)
                        print(f"ã‚¹ãƒãƒ›ç‰ˆã‹ã‚‰æŠ½å‡º: {name} - {height}cm")
                        break
        
        # ã‚ˆã‚Šå¤§ããªç¯„å›²ã§ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’æ¢ã™
        if not staff_list:
            all_text = soup.get_text()
            lines = all_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if 'cm' in line or 'ã' in line:
                    # è¡Œå…¨ä½“ã‹ã‚‰åå‰ã¨èº«é•·ã‚’æŠ½å‡º
                    name_height_patterns = [
                        r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+.*?(\d{3})cm',
                        r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+.*?(\d{3})ã',
                        r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+.*?èº«é•·[ï¼š:]\s*(\d{3})',
                        r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+.*?T[ï¼š:]\s*(\d{3})',
                        r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯\w\sâ™¡â˜†]+?)\s+.*?(\d{3})',
                    ]
                    
                    for pattern in name_height_patterns:
                        match = re.search(pattern, line)
                        if match:
                            name = match.group(1).strip()
                            height = int(match.group(2))
                            
                            if 140 <= height <= 200 and len(name) > 1 and len(name) < 20:
                                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                if not any(staff['åå‰'] == name for staff in staff_list):
                                    staff_info = {
                                        'åå‰': name,
                                        'èº«é•·': height,
                                        'ç”»åƒURL': '',
                                        'è©³ç´°': line,
                                        'å€‹åˆ¥URL': '',
                                        'ã‚½ãƒ¼ã‚¹': 'ã‚¹ãƒãƒ›ç‰ˆãƒ†ã‚­ã‚¹ãƒˆ'
                                    }
                                    staff_list.append(staff_info)
                                    print(f"ã‚¹ãƒãƒ›ç‰ˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º: {name} - {height}cm")
                                    
                                    # ä¸€æ˜¥ã®å ´åˆã¯ç‰¹åˆ¥ã«ãƒ­ã‚°å‡ºåŠ›
                                    if 'ä¸€æ˜¥' in name:
                                        print(f"ğŸ” ä¸€æ˜¥ç™ºè¦‹ï¼ å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {line}")
                                        print(f"ğŸ” æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern}")
                                        print(f"ğŸ” æŠ½å‡ºã•ã‚ŒãŸèº«é•·: {height}cm")
                                break
        
        return staff_list if staff_list else None
        
    except Exception as e:
        print(f"ã‚¹ãƒãƒ›ç‰ˆã‚¹ã‚¿ãƒƒãƒ•ä¸€è¦§æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_staff_details_from_links(staff_links, headers):
    """å€‹åˆ¥ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    try:
        all_staff = []
        
        for i, staff_link in enumerate(staff_links[:10]):  # æœ€åˆã®10åã®ã¿ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
            try:
                print(f"ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±å–å¾—ä¸­: {staff_link['name']} ({i+1}/{min(len(staff_links), 10)})")
                
                time.sleep(random.uniform(0.5, 1.0))
                response = requests.get(staff_link['url'], headers=headers, timeout=10)
                response.encoding = 'utf-8'
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’æŠ½å‡º
                    staff_text = soup.get_text()
                    height = extract_height(staff_text)
                    
                    # ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’æŠ½å‡º
                    profile_info = extract_profile_info(soup)
                    
                    # ç”»åƒURLã‚’æ¢ã™
                    img_url = ""
                    img_elem = soup.find('img', src=True)
                    if img_elem:
                        img_src = img_elem['src']
                        if img_src.startswith('/'):
                            img_url = f"https://www.host2.jp{img_src}"
                        elif img_src.startswith('http'):
                            img_url = img_src
                        else:
                            base_url = '/'.join(staff_link['url'].split('/')[:-1])
                            img_url = f"{base_url}/{img_src}"
                    
                    staff_info = {
                        'åå‰': staff_link['name'],
                        'ç”»åƒURL': img_url,
                        'èº«é•·': height,
                        'è©³ç´°': staff_text[:300] + '...' if len(staff_text) > 300 else staff_text,
                        'å€‹åˆ¥URL': staff_link['url'],
                        **profile_info
                    }
                    
                    all_staff.append(staff_info)
                    
                    if height:
                        print(f"  âœ“ èº«é•·: {height}cm")
                    else:
                        print(f"  âœ— èº«é•·æƒ…å ±ãªã—")
                        
                else:
                    print(f"  âœ— HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return all_staff if all_staff else None
        
    except Exception as e:
        print(f"ã‚¹ã‚¿ãƒƒãƒ•è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_profile_info(soup):
    """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    try:
        profile_info = {}
        
        # èª•ç”Ÿæ—¥ã€è¡€æ¶²å‹ã€æ˜Ÿåº§ãªã©ã®æƒ…å ±ã‚’æŠ½å‡º
        profile_patterns = {
            'èª•ç”Ÿæ—¥': [
                r'èª•ç”Ÿæ—¥[ï¼š:]\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'èª•ç”Ÿæ—¥[ï¼š:]\s*(\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'ç”Ÿå¹´æœˆæ—¥[ï¼š:]\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
            ],
            'è¡€æ¶²å‹': [
                r'è¡€æ¶²å‹[ï¼š:]\s*([ABO]å‹)',
                r'è¡€æ¶²å‹[ï¼š:]\s*([ABO]å‹)',
            ],
            'æ˜Ÿåº§': [
                r'æ˜Ÿåº§[ï¼š:]\s*([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯]+åº§)',
                r'æ˜Ÿåº§[ï¼š:]\s*([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯]+åº§)',
            ],
            'å¹´é½¢': [
                r'(\d{1,2})æ‰',
                r'(\d{1,2})æ­³',
                r'å¹´é½¢[ï¼š:]\s*(\d{1,2})',
            ]
        }
        
        text = soup.get_text()
        
        for info_type, patterns in profile_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, text)
                if match:
                    profile_info[info_type] = match.group(1)
                    break
        
        return profile_info
        
    except Exception as e:
        print(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def scrape_host_clubs_async():
    global scraping_status, scraping_results
    
    try:
        scraping_status['message'] = 'åº—èˆ—URLã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...'
        shop_urls = get_shop_urls()
        
        if not shop_urls:
            scraping_status['message'] = 'åº—èˆ—URLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚'
            scraping_status['is_running'] = False
            return
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        search_keyword = scraping_status.get('search_keyword', '')
        if search_keyword:
            filtered_urls = []
            for url in shop_urls:
                if search_keyword.lower() in url.lower():
                    filtered_urls.append(url)
            shop_urls = filtered_urls
            scraping_status['message'] = f'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ "{search_keyword}" ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(shop_urls)}ä»¶'
        
        scraping_status['total'] = len(shop_urls)
        scraping_status['message'] = f'{len(shop_urls)}ä»¶ã®åº—èˆ—URLã‚’å–å¾—ã—ã¾ã—ãŸã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...'
        
        clubs = []
        include_staff = scraping_status.get('include_staff', False)
        staff_success_count = 0
        staff_total_count = 0
        
        for i, url in enumerate(shop_urls, 1):
            if not scraping_status['is_running']:
                break
                
            scraping_status['progress'] = i
            scraping_status['current_url'] = url
            
            shop_info = get_shop_info(url, include_staff)
            if shop_info:
                clubs.append(shop_info)
                staff_count = len(shop_info.get('ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±', [])) if include_staff else 0
                if include_staff:
                    staff_total_count += 1
                    if staff_count > 0:
                        staff_success_count += 1
                scraping_status['message'] = f'æˆåŠŸ: {i}/{len(shop_urls)} ({len(clubs)}ä»¶å–å¾—æ¸ˆã¿, ã‚¹ã‚¿ãƒƒãƒ•{staff_count}å, ã‚¹ã‚¿ãƒƒãƒ•æˆåŠŸç‡: {staff_success_count}/{staff_total_count})'
            else:
                scraping_status['message'] = f'å¤±æ•—: {i}/{len(shop_urls)}'
        
        if clubs:
            # ã‚¤ãƒ™ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            event_keyword = scraping_status.get('event_keyword', '')
            if event_keyword:
                filtered_clubs = []
                for club in clubs:
                    # åº—èˆ—æƒ…å ±ã§ã‚¤ãƒ™ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
                    club_text = ' '.join([str(v) for v in club.values() if v and isinstance(v, str)])
                    if event_keyword.lower() in club_text.lower():
                        filtered_clubs.append(club)
                clubs = filtered_clubs
                scraping_status['message'] = f'ã‚¤ãƒ™ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ "{event_keyword}" ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(clubs)}ä»¶'
            
            # ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯åˆ¥ã‚·ãƒ¼ãƒˆã«ä¿å­˜
            if include_staff:
                # åº—èˆ—æƒ…å ±ã¨ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ã‚’åˆ†é›¢
                shop_data = []
                staff_data = []
                min_height = scraping_status.get('min_height')
                
                for club in clubs:
                    shop_info = {k: v for k, v in club.items() if k != 'ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±'}
                    shop_data.append(shop_info)
                    
                    if 'ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±' in club and club['ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±']:
                        for staff in club['ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±']:
                            # èº«é•·ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                            if min_height is not None:
                                staff_height = staff.get('èº«é•·')
                                if staff_height is None or staff_height < min_height:
                                    continue
                                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
                                print(f"èº«é•·ãƒ•ã‚£ãƒ«ã‚¿: {staff['åå‰']} - èº«é•·: {staff_height}cm (é–¾å€¤: {min_height}cm)")
                            
                            # ã‚¤ãƒ™ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±ï¼‰
                            if event_keyword:
                                staff_text = ' '.join([str(v) for v in staff.values() if v and isinstance(v, str)])
                                if event_keyword.lower() not in staff_text.lower():
                                    continue
                            
                            staff_info = {
                                'åº—èˆ—å': club['åº—èˆ—å'],
                                'åº—èˆ—URL': club['åº—èˆ—URL'],
                                **staff
                            }
                            staff_data.append(staff_info)
                
                # çµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜
                scraping_results = {
                    'shop_data': shop_data,
                    'staff_data': staff_data,
                    'timestamp': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
                }
                
                # Excelãƒ•ã‚¡ã‚¤ãƒ«ã«è¤‡æ•°ã‚·ãƒ¼ãƒˆã§ä¿å­˜
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f'host_clubs_{timestamp}.xlsx'
                
                with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                    pd.DataFrame(shop_data).to_excel(writer, sheet_name='åº—èˆ—æƒ…å ±', index=False)
                    if staff_data:
                        pd.DataFrame(staff_data).to_excel(writer, sheet_name='ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±', index=False)
            else:
                # é€šå¸¸ã®åº—èˆ—æƒ…å ±ã®ã¿
                df = pd.DataFrame(clubs)
                
                # çµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜
                scraping_results = {
                    'shop_data': clubs,
                    'staff_data': [],
                    'timestamp': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
                }
                
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f'host_clubs_{timestamp}.xlsx'
                df.to_excel(filename, index=False)
            
            scraping_status['filename'] = filename
            scraping_status['message'] = f'ãƒ‡ãƒ¼ã‚¿ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚åˆè¨ˆ {len(clubs)} ä»¶ã®åº—èˆ—æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚'
            scraping_status['completed'] = True
        else:
            scraping_status['message'] = 'åº—èˆ—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚'
    
    except Exception as e:
        scraping_status['message'] = f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
    
    finally:
        scraping_status['is_running'] = False

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/start_scraping', methods=['POST'])
def start_scraping():
    global scraping_status
    
    if scraping_status['is_running']:
        return jsonify({'status': 'error', 'message': 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯æ—¢ã«å®Ÿè¡Œä¸­ã§ã™ã€‚'})
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    data = request.get_json()
    search_keyword = data.get('search_keyword', '').strip()
    event_keyword = data.get('event_keyword', '').strip()
    include_staff = data.get('include_staff', False)
    min_height = data.get('min_height', None)
    
    # èº«é•·ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    if min_height is not None:
        try:
            min_height = int(min_height)
            if min_height < 100 or min_height > 200:
                return jsonify({'status': 'error', 'message': 'èº«é•·ã¯100cmã€œ200cmã®ç¯„å›²ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚'})
        except ValueError:
            return jsonify({'status': 'error', 'message': 'èº«é•·ã¯æ•°å€¤ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚'})
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
    scraping_status = {
        'is_running': True,
        'progress': 0,
        'total': 0,
        'current_url': '',
        'message': 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...',
        'filename': '',
        'completed': False,
        'search_keyword': search_keyword,
        'event_keyword': event_keyword,
        'include_staff': include_staff,
        'min_height': min_height
    }
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    thread = threading.Thread(target=scrape_host_clubs_async)
    thread.daemon = True
    thread.start()
    
    return jsonify({'status': 'success', 'message': 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚'})

@app.route('/stop_scraping', methods=['POST'])
def stop_scraping():
    global scraping_status
    scraping_status['is_running'] = False
    scraping_status['message'] = 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚'
    return jsonify({'status': 'success', 'message': 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚'})

@app.route('/get_status')
def get_status():
    return jsonify(scraping_status)

@app.route('/download/<filename>')
def download_file(filename):
    try:
        return send_file(filename, as_attachment=True)
    except FileNotFoundError:
        return jsonify({'status': 'error', 'message': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚'})

@app.route('/results')
def show_results():
    global scraping_results
    return render_template('results.html', results=scraping_results)

@app.route('/get_results')
def get_results():
    global scraping_results
    return jsonify(scraping_results)

@app.route('/debug_height/<path:url>')
def debug_height(url):
    """èº«é•·æŠ½å‡ºã®ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        # URLã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        import urllib.parse
        decoded_url = urllib.parse.unquote(url)
        
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã¨ã‚¹ã‚¿ãƒƒãƒ•ãƒšãƒ¼ã‚¸ã®ä¸¡æ–¹ã‚’è©¦ã™
        urls_to_try = [
            decoded_url,  # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸
            decoded_url.replace('/index.html', '/staff.html'),
            decoded_url.replace('/index.html', '/member.html'),
        ]
        
        # ã‚¹ãƒãƒ›ç‰ˆã¨PCç‰ˆã®ä¸¡æ–¹ã‚’è©¦ã™
        user_agents = [
            get_mobile_headers(),  # ã‚¹ãƒãƒ›ç‰ˆ
            headers  # PCç‰ˆ
        ]
        
        all_results = []
        
        for ua_headers in user_agents:
            for test_url in urls_to_try:
                try:
                    response = requests.get(test_url, headers=ua_headers, timeout=10)
                    response.encoding = 'utf-8'
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # èº«é•·ãŒå«ã¾ã‚Œãã†ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                        height_texts = []
                        for text in soup.stripped_strings:
                            if any(keyword in text.lower() for keyword in ['cm', 'ã', 'ã‚»ãƒ³ãƒ', 'èº«é•·', 't:', 'tï¼š']):
                                height_texts.append(text)
                        
                        # èº«é•·æŠ½å‡ºãƒ†ã‚¹ãƒˆ
                        test_results = []
                        for text in height_texts[:10]:  # æœ€åˆã®10å€‹ã®ã¿
                            height = extract_height(text)
                            test_results.append({
                                'text': text,
                                'extracted_height': height
                            })
                        
                        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ã‚¹ã‚¿ãƒƒãƒ•æƒ…å ±æŠ½å‡ºãƒ†ã‚¹ãƒˆ
                        main_staff = extract_staff_from_main_page(soup)
                        
                        # ã‚¹ãƒãƒ›ç‰ˆã‚¹ã‚¿ãƒƒãƒ•ãƒªã‚¹ãƒˆæŠ½å‡ºãƒ†ã‚¹ãƒˆ
                        mobile_staff = extract_mobile_staff_list(soup, test_url, ua_headers == get_mobile_headers())
                        
                        ua_type = "ã‚¹ãƒãƒ›ç‰ˆ" if ua_headers == get_mobile_headers() else "PCç‰ˆ"
                        all_results.append({
                            'url': test_url,
                            'user_agent': ua_type,
                            'status': 'success',
                            'height_texts': height_texts,
                            'test_results': test_results,
                            'main_staff': main_staff,
                            'mobile_staff': mobile_staff,
                            'html_preview': soup.get_text()[:1000]  # HTMLã®æœ€åˆã®1000æ–‡å­—
                        })
                    else:
                        ua_type = "ã‚¹ãƒãƒ›ç‰ˆ" if ua_headers == get_mobile_headers() else "PCç‰ˆ"
                        all_results.append({
                            'url': test_url,
                            'user_agent': ua_type,
                            'status': 'error',
                            'error': f'HTTP {response.status_code}'
                        })
                        
                except Exception as e:
                    ua_type = "ã‚¹ãƒãƒ›ç‰ˆ" if ua_headers == get_mobile_headers() else "PCç‰ˆ"
                    all_results.append({
                        'url': test_url,
                        'user_agent': ua_type,
                        'status': 'error',
                        'error': str(e)
                    })
        
        return jsonify({
            'original_url': decoded_url,
            'results': all_results
        })
        
    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000) 